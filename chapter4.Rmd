# 4 Clustering and classification
## Setup

First let's load the needed packages and some custom functions.

```{r}
# Packages
pacman::p_load(tidyverse)

# Custom functions
source("src/fig_setup.R")

# Run time
start.time <- Sys.time()
```


## 4.2 Get data

> ### Instructions
> Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. Details about the Boston dataset can be seen for example here. (0-1 points)

Save the data to an object and check its structure

```{r}
# Read data
data <- MASS::Boston

# Glance at the data
finalfit::finalfit_glimpse(data)$Continuous %>%
  select(-label) %>%
  knitr::kable()

# Uncomment for data description
# ?MASS::Boston
```

The [`Boston`](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) data contains information about suburbs in Boston, Massachusetts. It is from the `MASS` package and has 506 rows and 14 columns. Briefly, the data describe . 

- `crim`: per capita crime rate by town.
- `zn`: proportion of residential land zoned for lots over 25,000 sq.ft.
- `indus`: proportion of non-retail business acres per town.
- `chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- `nox`: nitrogen oxides concentration (parts per 10 million).
- `rm`: average number of rooms per dwelling.
- `age`: proportion of owner-occupied units built prior to 1940.
- `dis`: weighted mean of distances to five Boston employment centres.
- `rad`: index of accessibility to radial highways.
- `tax`: full-value property-tax rate per $10,000.
- `ptratio`: pupil-teacher ratio by town.
- `black`: the proportion of blacks by town.
- `lstat`: lower status of the population (percent).
- `medv`: median value of owner-occupied homes in $1000s.


**Source**

- Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. *J. Environ. Economics and Management* 5, 81–102.

- Belsley D.A., Kuh, E. and Welsch, R.E. (1980) *Regression Diagnostics. Identifying Influential Data and Sources of Collinearity.* New York: Wiley.


## 4.3 Visualize

> ### Instructions
> Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them. (0-2 points)

Let's start by exploring our data.

```{r fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
# "chas" to nominal
# data <- data %>%
#   mutate(
#     chas = case_when(
#       chas == 1 ~ "Bounds river",
#       T ~ "Does not bound river"
#     )
#   )

# Plot
data %>%
  GGally::ggpairs(
    lower = list(continuous = "points", size = .01),
    upper = list(continuous = GGally::wrap("cor", method = "spearman")),
  )
```

### Distributions

The distributions of `crim`, `zn`, `age`, `dis`, `pratio`, `black`, `lstat` are heavily skewed and it is questionable to apply any parametric statistics on them (without any transformations such as box-cox or ordernorm). Variables `indus`, `rad`, `tax` have *multimodal distributions* with two distinct peaks. The variable `chas` is a dichotomous nominal variable coded as 0 and 1.


### Relationships

The majority of the variables are intercorrelated. For example, there is a strong positive association between `nox` and `crim` ($\rho = 0.82$). `lstat` and `medv` are inversely correlated ($\rho = -0.85$)



## 4.4 Standardize

> ### Instructions
> Standardize the dataset and print out summaries of the scaled data. How did the variables change? Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)

Let's scale the data so that every variable has a mean $\overline{x} = 0$ and sd $σ = 1$. 

```{r warning=FALSE}
# Standardize
data_z <- scale(data) %>% data.frame()

# Cut into quartiles
quant <- 4
data_z <- data_z %>% mutate(
  crime_quant = gtools::quantcut(crim, q = 4, labels = paste0("Q", 1:quant))
) %>% select(-crim) # Remove original "crim"

# Summaries
summs <- finalfit::finalfit_glimpse(data_z) 
summs[[1]] %>% knitr::kable()
summs[[2]] %>% knitr::kable()
```

Let's divide 80% of the data to train and 20% test sets.

```{r}
# N of rows in the data
n <- nrow(data_z)

# Choose randomly 80% of the rows
ind <- sample(n, size = n * 0.8)

# Create train and test sets
train <- data_z[ind, ]
test <- data_z[-ind, ]
```


## 4.5 Discriminant analysis

> ### Instructions
> Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot. (0-3 points)

```{r fig.height=5, fig.width=7, warning=FALSE}
# Fit the LDA analysis on train set
lda.fit <- MASS::lda(crime_quant ~ ., data = train)
lda.fit

# Draw the LDA biplot
p <- ggord::ggord(
  lda.fit,
  train$crime_quant,
  size = 1,
  labcol = "white",
  veccol = "white",
  repel = T
)
# p <- p + ggsci::scale_color_lancet()
# p <- p + ggsci::scale_fill_lancet()
pal <- "PonyoMedium"
p <- p + ghibli::scale_color_ghibli_d(pal)
p <- p + ghibli::scale_fill_ghibli_d(pal)
p <- p + theme_darkmode()

# Print without white edges
grid::grid.newpage()
grid::grid.draw(grid::rectGrob(gp = grid::gpar(fill = backg)))
print(p, newpage = FALSE)

```


## 4.6 Class prediction

> ### Instructions
> Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)

```{r}
# Save the correct classes from test data
correct_classes <- test$crime_quant

# Predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
ctab <- table(correct = correct_classes, predicted = lda.pred$class)
ctab; ctab %>% prop.table() %>% round(2) #%>% {. * 100}
```

The confusion matrix above tells us that most classes were predicted correctly. Predictions for Q1 suburbs were spread broadly across three different quartiles, whereas most/all Q4 suburbs were correctly classified. There were some false positives as well as negatives.


```{r}
# Overall model accuracy
accuracy <- mean(lda.pred$class == correct_classes) %>% scales::percent()
accuracy

# Misclassification rate
misclass <- mean(lda.pred$class != correct_classes) %>% scales::percent()
misclass
```

In general, model accuracy is `r accuracy` and misclassification rate `r misclass`. Since the training and testing sets are randomly assigned, the results change every time the code is run. Crossvalidation could provide better credence for evaluating the model performance.


## 4.7 Clustering

> ### Instructions
> Reload the Boston dataset and standardize the dataset (we did not do this in the Exercise Set, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)

Let's scale the dataset and calculate Eucledian distances between the observations.

```{r}
# Reload the dataset
boston <- MASS::Boston

# Standardize
boston_z <- boston %>% scale() %>% data.frame()

# Calculate distances between observations
dist_eu <- dist(boston_z)
summary(dist_eu)
```



```{r fig.width=6, fig.height=3.5}
# Determine max N of clusters
k_max <- 10

# Calculate total within-cluster sum of squares (TWSS) for different cluster numbers
# Uncomment to test
# TWSS <- sapply(1:k_max, \ (k) kmeans(boston_z, centers = k)$tot.withinss)

# Iterate in a loop to acquire more precise estimates
results <- list()
for (i in 1:50) {
  TWSS <- sapply(1:k_max, \ (k) kmeans(boston_z, centers = k)$tot.withinss)
  results[[i]] <- data.frame(iteration = i, N = 1:length(TWSS), TWSS) 
} 
results <- results %>% bind_rows()


# Visualize 
results %>%
  ggplot(aes(x = N, y = TWSS, color = iteration)) +
  geom_point(size = .2) +
  geom_line(alpha = .2, aes(group = iteration), linewidth = .2) +
  # stat_summary(fun = mean, geom = "line") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = .2) +
  scale_x_continuous(breaks = 1:k_max) +
  xlab("Number of clusters") +
  ylab("Total of Within Cluster Sum of Squares") +
  ggtitle("Finding the optimal number of clusters") +
  scale_color_gradientn(colours = c("orange", "orangered", "purple"))
```

Based on 50 simulations, the most dramatic drop in TWSS is achieved when increasing the number of clusters from `1` to `2`. Only diminshing returns are achieved after this drop. Thus, we we will continue with `2` clusters. 


```{r fig.width=8, fig.height=6}
# Calculate k-means with optimal N of clusters
km <- kmeans(boston_z, centers = 2)

# Add clusters to data frame
boston_z <- boston_z %>%
  mutate(
    cluster = factor(km$cluster)
  )

# Plot
boston_z %>%
  reshape2::melt(id.vars = "cluster") %>%
  ggplot(aes(x = cluster, y = value, color = cluster)) +
  ggforce::geom_sina(size = .5, alpha = .7) +
  stat_summary(
    fun.data = mean_cl_normal,
    color = "white", geom = "errorbar", width = .2
  ) +
  ggpubr::stat_compare_means(
    vjust = 1, method = "wilcox.test",
    aes(x = 1.5, label = paste0("P=", after_stat(p.adj), 2))
  ) +
  theme(
    panel.grid.major.x = element_blank()
  ) +
  scale_color_manual(values = c("orange", "orangered2")) +
  xlab("Cluster") + ylab("Standard deviations") +
  facet_wrap(. ~ variable, ncol = 7)

```

The most clear separation between the clusters can be seen in variables `nox`, `tax`, `crim` and `indus`. P-values for between-cluster differences are crazy exponentially small, which highlights the power of k-mean clustering. A full scatter plot was also made with `GGally::ggpairs()`, but it was hard to read and narrowing it could omit important information.


## Bonus

> ### Instructions
> Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influential linear separators for the clusters? (0-2 points to compensate any loss of points from the above exercises)

Let's pick 4 clusters since it is reasonable and adding clusters only marginally decreases TWSS.

```{r}
# Reload and standardize
boston_z <- MASS::Boston %>% scale() %>% data.frame()

set.seed(14)
# Calculate k-means with optimal N of clusters
boston_z <- boston_z %>%
  mutate(cluster = factor(kmeans(boston_z, centers = 4)$cluster))

# Fit the LDA analysis
lda.fit <- MASS::lda(cluster ~ ., data = boston_z)
lda.fit

# Draw the LDA biplot
p <- ggord::ggord(
  lda.fit,
  boston_z$cluster,
  size = 1,
  labcol = "white",
  veccol = "white",
  repel = T
)
p <- p + ggsci::scale_color_lancet()
p <- p + ggsci::scale_fill_lancet()
p <- p + theme_darkmode()

# Print without white edges
grid::grid.newpage()
grid::grid.draw(grid::rectGrob(gp = grid::gpar(fill = backg)))
print(p, newpage = FALSE)



```

It appears that most of the times, `rad`, `indus` and `zn` are the most influential linear separators for the clusters. 

## Super-Bonus

> ### Instructions
> Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.

```{r}
# Use old train set that was analysed above
model_predictors <- train %>% select(-crime_quant)

# LDA model
lda.fit <- MASS::lda(crime_quant ~ ., data = train)

# Check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# Matrix multiplication
matrix_product <- (as.matrix(model_predictors) %*% lda.fit$scaling) %>%
  data.frame()
```

> Next, install and access the plotly package. Create a 3D plot (cool!) of the columns of the matrix product using the code below. Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities? (0-3 points to compensate any loss of points from the above exercises)

Let's first do the interactive 3D plot as **crime quartile** as the response variable.

```{r}
# Crime quartile as the response variable
plotly::plot_ly(
  x = matrix_product$LD1,
  y = matrix_product$LD2,
  z = matrix_product$LD3,
  type = 'scatter3d',
  mode = 'markers',
  color = train$crime_quant,
  colors = c("orange", "orangered", "purple", "dodgerblue2")
) %>%
  plotly::layout(
    font = list(color = 'white'),
    title = 'Crime quartile as the response variable',
    scene = list(
      xaxis = list(title = 'LD1'),
      yaxis = list(title = 'LD2'),
      zaxis = list(title = 'LD3')
    ),
    paper_bgcolor = backg
  )
```


Then let's change the response variable to K-mean clusters obtained earlier in this assignment.

```{r}
# Join
km_data <- train %>%
  rownames_to_column("id") %>%
  left_join(
    boston_z %>% rownames_to_column("id") %>% select(id, cluster),
    by = "id"
  ) %>% select(-id)
  
# K-mean cluster as the response variable
plotly::plot_ly(
  x = matrix_product$LD1,
  y = matrix_product$LD2,
  z = matrix_product$LD3,
  type = 'scatter3d',
  mode = 'markers',
  color = km_data$cluster,
  colors = c("orange", "orangered", "purple", "dodgerblue2")
) %>%
  plotly::layout(
    font = list(color = 'white'),
    title = 'K-mean cluster as the response variable',
    scene = list(
      xaxis = list(title = 'LD1'),
      yaxis = list(title = 'LD2'),
      zaxis = list(title = 'LD3')
    ),
    paper_bgcolor = backg
  )
```

The 3D plots seem almost identical irrespective of the grouping (color) variable (Crime quartile or K-mean clusters). There are only minor differences, such as a couple of blue dots in the purple cluster when using K-mean clusters to define colors. Fascinating!! The only possible explanation for the strikingly similar color patterns is that the `crim` variable is an important linear separator for the clusters and/or is intercorrelated to various other variables in the dataset.


```{r}
# Run time
end.time <- Sys.time()
elapsed.time <- (end.time - start.time) %>% round()
elapsed.time
```

