# 5 Dimensionality reduction techniques
## Setup

```{r}
# Packages
pacman::p_load(tidyverse)

# Custom functions
source("src/fig_setup.R")

# Run time
start.time <- Sys.time()
```


## 5.0 Get data

Automatically choose the newest version of the wrangled `human` dataset.

```{r message=FALSE, warning=FALSE}
# Read the most up-to-date data
datasets <- list.files("data/human/ready", full.names = T)
newest <- datasets %>% sub('.*_', '', .) %>% as.Date() %>% max()
data <- datasets[grepl(newest, datasets)] %>%
  readr::read_csv(show_col_types = F)
```


## 5.1 Inspect

Numerical and graphical overview:

```{r}
# Country to row names
data <- data %>%
  column_to_rownames("Country")

# Glance at the data
finalfit::finalfit_glimpse(data)$Continuous %>%
  select(-label) %>%
  knitr::kable()

# Visualize
GGally::ggpairs(
  data,
  upper = list(continuous = wrap("cor", method = "spearman"))
)
```


## 5.2 PCA using raw values

Let's deliberately make a principal component analysis with non-standardized variables.

```{r fig.width=5, fig.height=4}
# Principal component analysis
PCA <- prcomp(data, center = F, scale. = F)
PCA_s <- summary(PCA)
importances <- scales::percent(PCA_s$importance[2, ], .01)
importances

# Plot
factoextra::fviz_pca_biplot(
  PCA, repel = TRUE, col.var = "white", col.ind = "gray40", labelsize = 3,
) + theme_darkmode() +
  ggtitle("GNI in PC1 explains all variance")

```

**Problem:** Because the variables have differing scales, now variables with large absolute values dominate the GNI that is loaded in PC1 explains all the variability. This is not OK and we have to standardize the data asap. 


## 5.3 PCA using standardized values

A more optimal way to do principal component analysis is to have the variables on a similar measurement scale. (However there is still the limitation that various variables have skewed distributions).

```{r fig.width=5, fig.height=4}
# Standardize all variables
data_z <- scale(data) %>% data.frame()

# Principal component analysis
PCA <- prcomp(data_z, center = F, scale. = F)
PCA_s <- summary(PCA)
importances <- scales::percent(PCA_s$importance[2, ], .01)

# Plot
factoextra::fviz_pca_biplot(
  PCA, repel = TRUE, col.var = "white", col.ind = "gray40", labelsize = 3,
) + theme_darkmode() +
  ggtitle("Variables are now balanced")
```

As expected, results are much more balanced when variables are now measured at the same scale. Most influential variables include `Labo.FMm` `Mar.Mor` and `Ado.Birth`.


## 5.4 Personal interpretation

The 2nd PC shown above explains 16% of the variability in the data which is a good number. As always, most is explained by PC1 (54% here). Education and expected life span have negative loadings for the 1st PC. Maternal mortality and adolescent birth rate have positive loadings. These negatively and positively loaded variables have inverse correlation in the dataset, e.g., `Mat.Mor` and `Edu.Exp`: $r=0.7$.


## 5.5 MCA

Multiple Correspondence Analysis (MCA) is a generalization of principal component analysis when the variables of interest are categorical instead of quantitative. Good package `factoextra` for visualization: http://rpkgs.datanovia.com/factoextra/reference/fviz_mca.html.

We are applying MCA to Let's see what kind of data we are dealing with:

```{r fig.width=10, fig.height=10}
# Get data
tea <- read.csv("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/tea.csv", stringsAsFactors = TRUE)
glimpse(tea)#; View(tea) 

# Visualize
tea %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap("key", scales = "free", ncol = 8) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  ggtitle('Visualization of the "tea" data set')
```

It looks like `Age` variable is not truly categorical. Let's remove it and **conduct the MCA analysis**:

```{r}
# Drop problematic variable as it is not truly categorical
names(tea)[19] # Age
mca <- FactoMineR::MCA(tea[, -which(names(tea) == "age")], graph = FALSE)
summary(mca)

```


Plot MCA results:

```{r fig.width = 10, fig.height=4}
# "Boring" biplot
# factoextra::fviz_mca_biplot(
#   mca,
# ) + theme_darkmode()


# Contributions of variable levels to dimensions 1 and 2
figg <- \ (x) {
  theme_darkmode() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
}
ggpubr::ggarrange(
  factoextra::fviz_screeplot(mca, addlabels = TRUE, ylim = c(0, 20)) +
    theme_darkmode(),
  factoextra::fviz_contrib(mca, choice = "var", axes = 1, top = 15) + figg(),
  factoextra::fviz_contrib(mca, choice = "var", axes = 2, top = 15) + figg(),
  align = "hv",
  ncol = 3
)

# Look at interesting variables
factoextra::fviz_ellipses(
  mca, c("tearoom", "age_Q"),
  geom = "point"
) + theme_darkmode()
```

This MCA is trying to explain all the variables related to tea drinking in 300 participants. As there are a lot of variables, the proportion of variability that each dimension explains is relatively low, max. 5.8% for dimension 1. Distance between row points or column points in the MCA factor map tells about their similarity (or dissimilarity) -- similar column and row points are close to each other.

The bar plot shows the contribution of variables to principal dimensions. Variable `tearoom` has the strongest correlation with dimension 1 and `age_Q` has the strongest correlation to dimension 2.

```{r}
# Run time of this chapter code
end.time <- Sys.time(); elapsed.time <- (end.time - start.time) %>% round()
elapsed.time
```



