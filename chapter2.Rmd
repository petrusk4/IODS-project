# 2 Regression and model validation

This part describes the statistical analysis of `learning2014` dataset.

> ### Instructions
> Describe the work you have done this week and summarize your learning.
>
> - Describe your work and results clearly. 
> - Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
> - Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

## Setup

First let's load our packages and some custom functions.

```{r}
# Packages
pacman::p_load(tidyverse, GGally, patchwork, ggfortify)

## Defining some custom functions
# Colors
colorpair <- c("orangered", "dodgerblue")
color_gender <- \ () {scale_color_manual(values = colorpair)}
fill_gender <- \ () {scale_fill_manual(values = colorpair)}

# Dark mode theme
theme_darkmode <- \ (backg = "#141415") {
  ggdark::dark_theme_gray() %+replace% 
    ggplot2::theme(
      plot.background = element_rect(color = backg, fill = backg),
      legend.box.background = element_rect(fill = backg, color = backg),
      legend.background = element_rect(fill = backg, color = backg),
      panel.grid = element_line(color = "gray20")
    )
}

# Beautiful labels
labs <- tribble(
  ~var, ~lab,
  "age", "Age",
  "attitude", "Attitude",
  "points", "Exam points",
  "deep", "Deep learning",
  "stra", "Strategic learning",
  "surf", "Surface learning"
)
```


## 2.1 Get and explore data

> ### Instructions
> Read the students2014 data into R --.
> Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. 

Let's read in the `learning2014` dataset and check that it is read correctly.

```{r fig.width=7, fig.height=2}
# Read data
datasets <- list.files("data/learning/", full.names = T)
newest <- datasets %>% sub('.*_', '', .) %>% as.Date() %>% max()
data <- datasets[grepl(newest, datasets)] %>%
  readr::read_csv(show_col_types = F)

# Glance at the data
# data %>% head() %>% knitr::kable()
str(data, give.attr = F)

# Plot data missingness
naniar::vis_miss(data) + theme_darkmode() + ggtitle("Missingness map")
```

There are a total of **166 observations** and **7 variables**. We are dealing with **complete data** with no missing values. The variables can be divided to the types below (click here for more comprehensive [metadata](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-meta.txt)).

#### Population characteristics

- `age`: Age of the participant (in years) derived from the date of birth.
- `gender`: Here gender is coded as a nominal variable with two defined/prevalent values (F = Female, M = Male).


#### Survey answers

Clearly, the four variables `attitude`, `deep`, `stra`, and `surf` represent survey answers on a likert scale (1--5). `attitude` captures student's global attitude towards statistics. The rest of the variables were computed as averages of various interrelated questions and describe the traits below and also described in detail e.g., [here](https://spark.scu.edu.au/kb/tl/teach/focus-on-learning/deep-surface-and-strategic-learning). These traits are important as they may influence success in students' work, reflected by exam `points`.

- `surf`: *Surface learning*; emphasis upon memorising details to complete the assignment. Learning may be more superficial.
- `deep`: *Deep learning*; looking for the overall meaning and attempting to process information in a holistic way.
- `stra`: *Strategic learning*; organizing one's learning with the objective of achieving a positive outcome. Can involve a combination of both deep and surface learning strategies.


#### Exam results

- `points`: Results of the statistics exam. I did not find the maximum possible value from the metadata. Before the analysis, students that did not attend the exam (points == 0) were excluded from the dataset.

To note, these data lack subject identifier (ID) variable. However, it may not be needed as we do not have repeated measures. 


## 2.2 Graphical overwiew

> ### Instructions
> Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them. (0-3 points)

Let's visualize the distributions of our dataset with `ggplot2`.

```{r fig.height=4, fig.width=7, message=FALSE, warning=FALSE}
p <- data %>% pivot_longer(
  cols = colnames(data)[-1],
  values_to = "value",
  names_to = "var"
) %>%
  left_join(labs) %>%
  ggplot(aes(x = value, color = lab, fill = lab, y = lab, linetype = gender))
p <- p + ggridges::geom_density_ridges(
  aes(point_shape = gender),
  rel_min_height = .001, quantile_lines = T, quantiles = .5,
  jittered_points = TRUE, position = "raincloud",
  point_alpha = .5, point_size = 2, alpha = .3
)
p <- p + scale_shape_manual(values = c(0, 2))

breakx <- c(1, 5, seq(10, 60, by = 10))
p <- p + scale_x_sqrt(breaks = breakx, labels = breakx)
p <- p + labs(
  title = "Distributions by gender",
  subtitle = "(On a square root x-axis)"
) + xlab("Value") + ylab("")
p
```

**Likert-scale variables** are quite evenly spread on the range of 1--5. It appears that these students are `deep` learners more so than surface learners (`surf`). By visual inspection, men have more positive `attitude` towards statistics than women. Students tend to be of quite **young** `age` although a few older students skew the distribution heavily to the right. It also seems like men may be slightly older than female students. Whether these differences are statistically significant would require further testing...


### Numerical summaries

Let's summarize the numerical variables. 

```{r}
# Summaries for numeric variables
do.call(cbind, lapply(data, summary)) %>%
  data.frame() %>% select(-gender) %>%
  mutate(across(1:6, \ (x) as.numeric(x))) %>%
  mutate(across(1:6, \ (x) round(x, 1))) %>%
  t() %>% knitr::kable() # DT::datatable()
```

Let's also see the distribution of `gender`. It appears that most students (66%) are female. On average, male students are `r round(mean(data$age[which(data$gender == "M")]) - mean(data$age[which(data$gender == "F")]), 1)` years older than female students. Later we'll probably also see whether `gender` modifies the relationship between `attitude`/learning and exam `points`.

```{r}
data %>% group_by(gender) %>%
  summarise(
    n = sum(!is.na(gender)),
    age_mean_sd = paste0(round(mean(age), 1), " (", round(sd(age), 1), ")")
  ) %>%
  mutate(pct = scales::percent(n / sum(n))) %>%
  select(gender, n, pct, everything()) %>%
  knitr::kable()
```


### Predictors of exam `points`

It is plausibile that students' `attitude` or learning strategies influence exam `points`. Therefore, let's glance over these potential relationships:

```{r fig.width=7, fig.height=4}
p <- data %>% pivot_longer(
  cols = c(attitude, deep, stra, surf),
  values_to = "answer",
  names_to = "var"
) %>%
  left_join(labs) %>%
  ggplot(aes(x = answer, y = points, color = lab))
p <- p + geom_point(alpha = .5, size = .5)
p <- p + geom_smooth(method = "lm", se = F)
p <- p + coord_cartesian(xlim = c(1, 5))
p <- p + labs(title = "Predictors for good or bad exam success") +
  xlab("Average answer for the set of questions") + ylab("Exam points")
p
```

Intuitively and as hypothesized, `attitude` shows positive relationship with exam `points`, whereas `surf`ace learning may be negatively associated with `points`. The first does not however imply that attitude is *causal* for success, as there could be a confounder (e.g., competence) that associates with attitude and *causes* good exam points. The link between surface learning and poor exam points is surprising, as surface learning is focused precisely around the exam. However, whether these associations are statistically significant requires testing!


### Gender differences

It could be hypothesized that attitude, learning strategies and/or exam points differ between the `gender`s. Let's also do some preliminary hypothesis testing using Wilcoxon signed-ranks test that requires minimal assumptions.

```{r fig.height=4, fig.width=8, message=FALSE, warning=FALSE}
# Gender bar plot
plotdata <- data %>%
  group_by(gender) %>%
  reframe(n = n()) %>%
  mutate(
    pct = scales::percent(n / sum(n))
  )
# Calculate 95% CIs
for (i in 1:nrow(plotdata)) {
  plotdata$Lower[i] <- Hmisc::binconf(
    x = plotdata$n[i], n = sum(plotdata$n)
  )[[2]] * sum(plotdata$n)
  plotdata$Upper[i] <- Hmisc::binconf(
    x = plotdata$n[i], n = sum(plotdata$n)
  )[[3]] * sum(plotdata$n)
}
# Plot
p <- plotdata %>%
  ggplot(aes(x = gender, y = n, fill = gender))
p <- p + geom_col(width = .5)
p <- p + geom_errorbar(width = .2, aes(ymin = Lower, ymax = Upper))
p <- p + geom_text(aes(y = Upper, label = paste0(pct, "\n")))
p <- p + fill_gender()
p <- p + scale_y_continuous(expand = expansion(mult = c(0, .1)))
p <- p + labs(title = "Gender", subtitle = "95% CI")
p1 <- p

# Facetted sina plots
p <- data %>% pivot_longer(
  cols = colnames(data)[-1],
  values_to = "value",
  names_to = "var"
) %>%
  left_join(labs) %>%
  ggplot(aes(y = value, x = gender, color = gender))
p <- p + geom_violin(fill = "white", color = NA, alpha = .1)
p <- p + ggforce::geom_sina(size = .5)
p <- p + geom_boxplot(width = .2, color = "white", outlier.color = NA, alpha = .5)
p <- p + stat_summary(fun = mean, shape = 4, color = "white")
p <- p + facet_wrap(. ~ lab, scales = "free")
p <- p + color_gender()
p <- p + labs(
  title = "Potential gender differences",
  subtitle = "Wilcoxon signed-ranks test"
)
p <- p + ggpubr::stat_compare_means(
  method = "wilcox.test", size = 3,
  label = "p.format", hjust = .5, aes(x = 1.5)
)
p <- p + scale_y_continuous(expand = expansion(mult = c(0, .15)))
p2 <- p

ggpubr::ggarrange(p1, p2, legend = "none", widths = c(.3, 1), labels = "AUTO")
```

As observed from the plots, male students are the minority on the course and they are significantly older than female students. Men have significantly more positive attitude on statistics. Strategic learning does not differ between genders despite quite small $p$ value.


### Influence of gender (interactions)

Perhaps `gender` modifies the relationship between learning strategies and exam points -- let's entertain this idea (without making regression models with interactions yet). 

```{r fig.height=6, fig.width=10, message=FALSE}
# Define independent variables
traits <- tribble(
  ~var, ~lab, ~unit, ~zoomx,
  "age", "age", "Years", c(10, 60),
  "attitude", "attitude", "Averaged answer", c(1, 5),
  "deep", "deep learning", "Averaged answer", c(1, 5),
  "stra", "strategic learning", "Averaged answer", c(1, 5),
  "surf", "surface learning", "Averaged answer", c(1, 5)
)

# Plot in a loop
plist <- lapply(1:nrow(traits), function (i) {
  p <- data %>%
    ggplot(aes(x = .data[[traits$var[i]]], y = points, col = gender, fill = gender))
  p <- p + geom_point(size = .5, alpha = .5)
  p <- p + geom_smooth(method = "lm")
  p <- p + ggtitle(paste0("Student's ", traits$lab[[i]], "\nversus exam points"))
  p <- p + coord_cartesian(xlim = traits$zoomx[[i]])
  p <- p + xlab(traits$unit[[i]]) + ylab("Exam points")
  p <- p + color_gender() + fill_gender()
  p
})
# Combine plots into a panel
plist[[1]] + plist[[2]] + plist[[3]] + plist[[4]] + plist[[5]] +
  plot_layout(guides = "collect")
```

The slopes for men and women seem quite similar. If anything, aging might hinder men's exam success, whereas women do not experience age-related decline in exam points.



Lets make a "monster plot" of all the relationships within the data.

```{r message=FALSE}
# Visualize
# pairs(data[-1])
data %>% GGally::ggpairs(
  aes(col = gender, alpha = .3),
  lower = list(continuous = "smooth")
) +
  color_gender() + fill_gender()
```

Here it is demonstrated that the strongest correlation is observed between `attitude` and `points`. In men, there is a strong inverse correlation between surface and deep learning, whereas such phenomenon doesn't exist in women. This implies that women may employ both strategies simultaneously, whereas in men these two strategies may be mutually exclusive. 


## 2.3 Regression model

> ### Instructions
> Choose three variables as explanatory variables and fit a regression model where exam points is the target (dependent, outcome) variable. Show a summary of the fitted model and comment and interpret the results. Explain and interpret the statistical test related to the model parameters.

Next I will fit a regression model that looks at the association between `attitude` and exam `points` adjusted for `age` and `gender`.

```{r}
# Define formula
response <- "points"
IVs <- c("attitude", "age", "gender")
formula <- paste0(response, " ~ ", paste(IVs, collapse = " + "))

# Run initial model
m1 <- lm(formula, data)
m1_s <- summary(m1)
m1_s
```

> ### Instructions
> If an explanatory variable in your model does not have a statistically significant relationship with the target variable, remove the variable from the model and fit the model again without it.

Only `attitude` is significant in our model. Let's remove non-significant predictors one by one and do it efficiently in a loop.

```{r}
# Drop useless covariates in a loop
results <- list(); models <- list()
while (max(m1_s$coefficients[-1, "Pr(>|t|)"]) > 0.05) {
  
  # Identify the most non-significant covariate
  nonsig <- with(m1_s, rownames(coefficients)[
    coefficients[, "Pr(>|t|)"] == max(coefficients[, "Pr(>|t|)"])
  ]); nonsigF <- nonsig # Save "raw" covariate with factor level
  
  # Remove factor value characters
  while (!(nonsig %in% colnames(data))) { 
    nonsig <- substr(nonsig, 1, nchar(nonsig)-1)
  }
  # Store non-significant covariate
  results[[nonsigF]] <- data.frame(
    Dropped = nonsigF,
    P = m1_s$coefficients[nonsigF, "Pr(>|t|)"] %>% signif(3)
  )
  # Remove covariate from model
  IVs <- IVs[!(IVs %in% nonsig)]
  
  # Quit if no predictors left
  if (length(IVs) == 0) {print("All predictors NS"); break}
  if (length(IVs) > 0) {
    formula <- paste0(response, " ~ ", paste(IVs, collapse = " + "))
    m1 <- lm(formula, data); m1_s <- summary(m1)
  } 
}
dropped <- bind_rows(results); dropped %>% knitr::kable()
```

Both `gender` and `age` were dropped out one by one due to their non-significant contribution to the model. Let's see if this this decision replicates with official `rms::fastbw` function for backward variable selection:

```{r}
# Run initial model
m_ols <- rms::ols(points ~ attitude + age + gender, data)

# Backward variable selection based on p value
rms::fastbw(m_ols, rule = "p")
```

Yup, we got the same result -- the only predictor left in the model is `attitude.` Thus, in the final model we'll predict exam `points` with only `attitude`.


## 2.4 Interpretation

> ### Instructions
> Using a summary of your fitted model, explain the relationship between the chosen explanatory variables and the target variable (interpret the model parameters). Explain and interpret the multiple R-squared of the model. (0-3 points)

Let's look at the final model:

```{r}
# Final model
m1_s
```

In this model, coefficient for `attitude` is 3.5, which means that each one unit increase in `attitude` corresponds to 3.5 `points` higher exam score. The intercept means that in case `attitude` was zero, the amount of exam `points` would be 11.6. However, this is only theoretical as zero is outside of the likert scale's range (1--5).

R-squared values (both multiple and adjusted = 0.19) show that only a small amount of variation (19%) in exam score can be explained by attitude. This is understandable as a lot of other things, and also our methods of measuring attitude could be imperfect.


## 2.5 Graphical model validation

> ### Instructions
> Produce the following diagnostic plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage. Explain the assumptions of the model and interpret the validity of those assumptions based on the diagnostic plots

Linear regression has a few key assumptions:

- **Linear** relationship between the predictor `x` and response `y`.
- **Normality** of the residuals.
- **Homoscedasticity**: the residuals are assumed to have constant variance.

Let's see if our assumptions hold.

```{r fig.width=6, fig.height=5}
# Residuals
model.diag.metrics <- broom::augment(m1)
ggplot(model.diag.metrics, aes(attitude, points)) +
  geom_segment(
    aes(xend = attitude, yend = .fitted),
    color = "orangered2", alpha = .7
  ) +
  geom_point() +
  stat_smooth(color = "white", method = lm, se = FALSE)

# The actual diagnostic plots
m1_diag <- ggplot2::autoplot(m1, colour = "white", size = .5)
m1_diag
```


- The **"Residuals vs Fitted"** plot shows a horizontal line, without distinct patterns, which indicates that the predictor and response variables have a linear relationship.
- In the **Normal Q-Q** plot, standardized residuals roughly follow the dashed line and deviate only at extreme quantiles.
- The **Scale-Location** plot shows that residuals spread roughly equally along the range of the predictor, indicating that variances are constant.
- The **Residuals vs Leverage** plot identifies two observations that have standardized residuals greater than absolute value 3, which are possible outliers.

To conclude, the model assumptions hold up quite good.
